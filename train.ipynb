{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/lustre/home/hyin/anaconda3/envs/paddle242/lib/python3.10/site-packages/setuptools/sandbox.py:14: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  import pkg_resources\n",
      "/lustre/home/hyin/anaconda3/envs/paddle242/lib/python3.10/site-packages/pkg_resources/__init__.py:2832: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\n",
      "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "  declare_namespace(pkg)\n",
      "/lustre/home/hyin/anaconda3/envs/paddle242/lib/python3.10/site-packages/paddle/fluid/framework.py:634: UserWarning: You are using GPU version Paddle, but your CUDA device is not set properly. CPU device will be used by default.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import paddle\n",
    "import paddle.nn as nn\n",
    "import pgl\n",
    "from scipy.spatial.distance import cdist\n",
    "import pandas\n",
    "from paddle.io import Dataset\n",
    "from dataloader import pygmmdataLoader\n",
    "# from dataloader import pygmmdataLoader\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "args=  {\n",
    "                \"task_name\": \"forecast\",\n",
    "                \"output_attention\": True,\n",
    "                \"seq_len\": 72,\n",
    "                \"label_len\": 24,\n",
    "                \"pred_len\": 48,\n",
    "\n",
    "                \"aq_gat_node_features\" : 7,\n",
    "                \"aq_gat_node_num\": 35,\n",
    "\n",
    "                \"mete_gat_node_features\" : 7,\n",
    "                \"mete_gat_node_num\": 18,\n",
    "\n",
    "                \"gat_hidden_dim\": 32,\n",
    "                \"gat_edge_dim\": 3,\n",
    "                \"gat_embed_dim\": 32,\n",
    "\n",
    "                \"e_layers\": 1,\n",
    "                \"enc_in\": 32,\n",
    "                \"dec_in\": 7,\n",
    "                \"c_out\": 7,\n",
    "                \"d_model\": 16 ,\n",
    "                \"embed\": \"fixed\",\n",
    "                \"freq\": \"t\",\n",
    "                \"dropout\": 0.05,\n",
    "                \"factor\": 3,\n",
    "                \"n_heads\": 4,\n",
    "\n",
    "                \"d_ff\": 32 ,\n",
    "                \"num_kernels\": 6,\n",
    "                \"top_k\": 4\n",
    "            }\n",
    "\n",
    "dataLoader_args = {\n",
    "            \"data_dir\": \"data/2020-2023_new/train_data.pkl\",\n",
    "            \"batch_size\": 32,\n",
    "            \"shuffle\": True,\n",
    "            \"num_workers\": 0,\n",
    "            \"training\": True\n",
    "        }\n",
    "args = argparse.Namespace(**args)\n",
    "\n",
    "paddle.device.set_device('cpu')\n",
    "dataloader = pygmmdataLoader(args,**dataLoader_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, targets = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(shape=[1], dtype=int64, place=Place(cpu), stop_gradient=True,\n",
       "       [2304])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aq_G = features['aq_G']\n",
    "aq_G.num_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0507 11:16:17.980007 147923 dynamic_loader.cc:307] The third-party dynamic library (libcudnn.so) that Paddle depends on is not configured correctly. (error code is /usr/local/cuda/lib64/libcudnn.so: cannot open shared object file: No such file or directory)\n",
      "  Suggestions:\n",
      "  1. Check if the third-party dynamic library (e.g. CUDA, CUDNN) is installed correctly and its version is matched with paddlepaddle you installed.\n",
      "  2. Configure third-party dynamic library environment variables as follows:\n",
      "  - Linux: set LD_LIBRARY_PATH by `export LD_LIBRARY_PATH=...`\n",
      "  - Windows: set PATH by `set PATH=XXX;\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done. \n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "import paddle\n",
    "import paddle.nn as nn\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "from math import sqrt\n",
    "from Embed import DataEmbedding\n",
    "# from pgl.nn import GATv2Conv\n",
    "\n",
    "class Inception_Block_V1(paddle.nn.Layer):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, num_kernels=6,\n",
    "        init_weight=True):\n",
    "        super(Inception_Block_V1, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.num_kernels = num_kernels\n",
    "        kernels = []\n",
    "        for i in range(self.num_kernels):\n",
    "            kernels.append(paddle.nn.Conv2D(in_channels=in_channels,\n",
    "                out_channels=out_channels, kernel_size=2 * i + 1, padding=i))\n",
    "        self.kernels = paddle.nn.LayerList(sublayers=kernels)\n",
    "        if init_weight:\n",
    "            self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.sublayers():\n",
    "            if isinstance(m, paddle.nn.Conv2D):\n",
    "\n",
    "                init_kaimingNormal = paddle.nn.initializer.KaimingNormal(fan_in=None,\n",
    "                            negative_slope=0.0, nonlinearity='relu')\n",
    "                init_kaimingNormal(m.weight)\n",
    "                \n",
    "                if m.bias is not None:\n",
    "                    init_Constant = paddle.nn.initializer.Constant(value=0)\n",
    "                    init_Constant(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        res_list = []\n",
    "        for i in range(self.num_kernels):\n",
    "            res_list.append(self.kernels[i](x))\n",
    "        res = paddle.stack(x=res_list, axis=-1).mean(axis=-1)\n",
    "        return res\n",
    "\n",
    "class AttentionLayer(paddle.nn.Layer):\n",
    "\n",
    "    def __init__(self, attention, d_model, n_heads, d_keys=None, d_values=None\n",
    "        ):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        d_keys = d_keys or d_model // n_heads\n",
    "        d_values = d_values or d_model // n_heads\n",
    "        self.inner_attention = attention\n",
    "        self.query_projection = paddle.nn.Linear(in_features=d_model,\n",
    "            out_features=d_keys * n_heads)\n",
    "        self.key_projection = paddle.nn.Linear(in_features=d_model,\n",
    "            out_features=d_keys * n_heads)\n",
    "        self.value_projection = paddle.nn.Linear(in_features=d_model,\n",
    "            out_features=d_values * n_heads)\n",
    "        self.out_projection = paddle.nn.Linear(in_features=d_values *\n",
    "            n_heads, out_features=d_model)\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "    def forward(self, queries, keys, values, attn_mask):\n",
    "        B, L, _ = tuple(queries.shape)\n",
    "        _, S, _ = tuple(keys.shape)\n",
    "        H = self.n_heads\n",
    "        queries = self.query_projection(queries).view(B, L, H, -1)\n",
    "        keys = self.key_projection(keys).view(B, S, H, -1)\n",
    "        values = self.value_projection(values).view(B, S, H, -1)\n",
    "        out, attn = self.inner_attention(queries, keys, values, attn_mask)\n",
    "        out = out.view(B, L, -1)\n",
    "        return self.out_projection(out), attn\n",
    "\n",
    "class ProbAttention(paddle.nn.Layer):\n",
    "\n",
    "    def __init__(self, mask_flag=True, factor=5, scale=None,\n",
    "        attention_dropout=0.1, output_attention=False):\n",
    "        super(ProbAttention, self).__init__()\n",
    "        self.factor = factor\n",
    "        self.scale = scale\n",
    "        self.mask_flag = mask_flag\n",
    "        self.output_attention = output_attention\n",
    "        self.dropout = paddle.nn.Dropout(p=attention_dropout)\n",
    "\n",
    "    def _prob_QK(self, Q, K, sample_k, n_top):\n",
    "        B, H, L_K, E = tuple(K.shape)\n",
    "        _, _, L_Q, _ = tuple(Q.shape)\n",
    "        K_expand = K.unsqueeze(axis=-3).expand(shape=[B, H, L_Q, L_K, E])\n",
    "        index_sample = paddle.randint(low=0, high=L_K, shape=(L_Q, sample_k))\n",
    "        K_sample = K_expand[:, :, paddle.arange(end=L_Q).unsqueeze(axis=1),\n",
    "            index_sample, :]\n",
    "        x = K_sample\n",
    "        perm_5 = list(range(x.ndim))\n",
    "        perm_5[-2] = -1\n",
    "        perm_5[-1] = -2\n",
    "        Q_K_sample = paddle.matmul(x=Q.unsqueeze(axis=-2), y=x.transpose(\n",
    "            perm=perm_5)).squeeze()\n",
    "        M = Q_K_sample.max(-1)[0] - paddle.divide(x=Q_K_sample.sum(axis=-1),\n",
    "            y=paddle.to_tensor(L_K))\n",
    "        M_top = M.topk(k=n_top, sorted=False)[1]\n",
    "        Q_reduce = Q[paddle.arange(end=B)[:, None, None], paddle.arange(end\n",
    "            =H)[None, :, None], M_top, :]\n",
    "        x = K\n",
    "        perm_6 = list(range(x.ndim))\n",
    "        perm_6[-2] = -1\n",
    "        perm_6[-1] = -2\n",
    "        Q_K = paddle.matmul(x=Q_reduce, y=x.transpose(perm=perm_6))\n",
    "        return Q_K, M_top\n",
    "\n",
    "    def _get_initial_context(self, V, L_Q):\n",
    "        B, H, L_V, D = tuple(V.shape)\n",
    "        if not self.mask_flag:\n",
    "            V_sum = V.mean(axis=-2)\n",
    "            contex = V_sum.unsqueeze(axis=-2).expand(shape=[B, H, L_Q,\n",
    "                tuple(V_sum.shape)[-1]]).clone()\n",
    "        else:\n",
    "            assert L_Q == L_V\n",
    "            contex = V.cumsum(axis=-2)\n",
    "        return contex\n",
    "\n",
    "    def _update_context(self, context_in, V, scores, index, L_Q, attn_mask):\n",
    "        B, H, L_V, D = tuple(V.shape)\n",
    "        if self.mask_flag:\n",
    "            attn_mask = ProbMask(B, H, L_Q, index, scores, device=V.place)\n",
    "            scores.masked_fill_(mask=attn_mask.mask, value=-np.inf)\n",
    "        attn = paddle.nn.functional.softmax(x=scores, axis=-1)\n",
    "        context_in[paddle.arange(end=B)[:, None, None], paddle.arange(end=H\n",
    "            )[None, :, None], index, :] = paddle.matmul(x=attn, y=V).astype(\n",
    "            dtype=context_in.dtype)\n",
    "        if self.output_attention:\n",
    "            attns = (paddle.ones(shape=[B, H, L_Q, L_V]) / L_V).astype(dtype\n",
    "                =attn.dtype).to(attn.place)\n",
    "            attns[paddle.arange(end=B)[:, None, None], paddle.arange(end=H)\n",
    "                [None, :, None], index, :] = attn\n",
    "            return context_in, attns\n",
    "        else:\n",
    "            return context_in, None\n",
    "\n",
    "    def forward(self, queries, keys, values, attn_mask):\n",
    "        B, L_Q, H, D = tuple(queries.shape)\n",
    "        _, L_K, _, _ = tuple(keys.shape)\n",
    "        x = queries\n",
    "        perm_7 = list(range(x.ndim))\n",
    "        perm_7[2] = 1\n",
    "        perm_7[1] = 2\n",
    "        queries = x.transpose(perm=perm_7)\n",
    "        x = keys\n",
    "        perm_8 = list(range(x.ndim))\n",
    "        perm_8[2] = 1\n",
    "        perm_8[1] = 2\n",
    "        keys = x.transpose(perm=perm_8)\n",
    "        x = values\n",
    "        perm_9 = list(range(x.ndim))\n",
    "        perm_9[2] = 1\n",
    "        perm_9[1] = 2\n",
    "        values = x.transpose(perm=perm_9)\n",
    "        U_part = self.factor * np.ceil(np.log(L_K)).astype('int').item()\n",
    "        u = self.factor * np.ceil(np.log(L_Q)).astype('int').item()\n",
    "        U_part = U_part if U_part < L_K else L_K\n",
    "        u = u if u < L_Q else L_Q\n",
    "        scores_top, index = self._prob_QK(queries, keys, sample_k=U_part,\n",
    "            n_top=u)\n",
    "        scale = self.scale or 1.0 / sqrt(D)\n",
    "        if scale is not None:\n",
    "            scores_top = scores_top * scale\n",
    "        context = self._get_initial_context(values, L_Q)\n",
    "        context, attn = self._update_context(context, values, scores_top,\n",
    "            index, L_Q, attn_mask)\n",
    "        return context, attn\n",
    "\n",
    "\n",
    "def FFT_for_Period(x, k=2):\n",
    "    xf = paddle.fft.rfft(x=x, axis=1)\n",
    "    frequency_list = abs(xf).mean(axis=0).mean(axis=-1)\n",
    "    frequency_list[0] = 0\n",
    "    _, top_list = paddle.topk(k=k, x=frequency_list)\n",
    "    top_list = top_list.detach().cpu().numpy()\n",
    "    period = tuple(x.shape)[1] // top_list\n",
    "    return period, abs(xf).mean(axis=-1)[:, top_list]\n",
    "\n",
    "\n",
    "class TimesBlock(paddle.nn.Layer):\n",
    "\n",
    "    def __init__(self, configs):\n",
    "        super(TimesBlock, self).__init__()\n",
    "        self.seq_len = configs.seq_len\n",
    "        self.pred_len = configs.pred_len\n",
    "        self.k = configs.top_k\n",
    "        self.conv = paddle.nn.Sequential(Inception_Block_V1(configs.d_model,\n",
    "            configs.d_ff, num_kernels=configs.num_kernels), paddle.nn.GELU(\n",
    "            ), Inception_Block_V1(configs.d_ff, configs.d_model,\n",
    "            num_kernels=configs.num_kernels))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, N = tuple(x.shape)\n",
    "        period_list, period_weight = FFT_for_Period(x, self.k)\n",
    "        res = []\n",
    "        for i in range(self.k):\n",
    "            period = period_list[i]\n",
    "            if (self.seq_len + self.pred_len) % period != 0:\n",
    "                length = ((self.seq_len + self.pred_len) // period + 1\n",
    "                    ) * period\n",
    "                padding = paddle.zeros(shape=[tuple(x.shape)[0], length - (\n",
    "                    self.seq_len + self.pred_len), tuple(x.shape)[2]]).to(x\n",
    "                    .place)\n",
    "                out = paddle.concat(x=[x, padding], axis=1)\n",
    "            else:\n",
    "                length = self.seq_len + self.pred_len\n",
    "                out = x\n",
    "            out = out.reshape(B, length // period, period, N).transpose(perm\n",
    "                =[0, 3, 1, 2])\n",
    "            out = self.conv(out)\n",
    "            out = out.transpose(perm=[0, 2, 3, 1]).reshape(B, -1, N)\n",
    "            res.append(out[:, :self.seq_len + self.pred_len, :])\n",
    "        res = paddle.stack(x=res, axis=-1)\n",
    "        period_weight_raw = period_weight\n",
    "        period_weight = paddle.nn.functional.softmax(x=period_weight, axis=1)\n",
    "        period_weight = period_weight.unsqueeze(axis=1).unsqueeze(axis=1\n",
    "            ).repeat(1, T, N, 1)\n",
    "        res = paddle.sum(x=res * period_weight, axis=-1)\n",
    "        res = res + x\n",
    "        return res, period_list, period_weight_raw\n",
    "\n",
    "\n",
    "class Gat_TimesNet_mm(paddle.nn.Layer):\n",
    "    \"\"\"\n",
    "    Paper link: https://openreview.net/pdf?id=ju_Uqw384Oq\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, configs, **kwargs):\n",
    "        super(Gat_TimesNet_mm, self).__init__()\n",
    "        # configs = argparse.Namespace(**configs)\n",
    "        self.device = str('cuda').replace('cuda', 'gpu')\n",
    "        self.configs = configs\n",
    "        self.task_name = configs.task_name\n",
    "        if hasattr(configs, 'output_attention'):\n",
    "            self.output_attention = configs.output_attention\n",
    "        else:\n",
    "            self.output_attention = False\n",
    "        self.seq_len = configs.seq_len\n",
    "        self.label_len = configs.label_len\n",
    "        self.pred_len = configs.pred_len\n",
    "        self.dec_in = configs.dec_in\n",
    "        self.gat_embed_dim = configs.enc_in\n",
    "        self.enc_embedding = DataEmbedding(configs.enc_in, configs.d_model,\n",
    "            configs.embed, configs.freq, configs.dropout)\n",
    "        self.aq_gat_node_num = configs.aq_gat_node_num\n",
    "        self.aq_gat_node_features = configs.aq_gat_node_features\n",
    "        self.aq_GAT = GAT_Encoder(configs.aq_gat_node_features, configs.\n",
    "            gat_hidden_dim, configs.gat_edge_dim, self.gat_embed_dim,\n",
    "            configs.dropout).to(self.device)\n",
    "        self.mete_gat_node_num = configs.mete_gat_node_num\n",
    "        self.mete_gat_node_features = configs.mete_gat_node_features\n",
    "        self.mete_GAT = GAT_Encoder(configs.mete_gat_node_features, configs\n",
    "            .gat_hidden_dim, configs.gat_edge_dim, self.gat_embed_dim,\n",
    "            configs.dropout).to(self.device)\n",
    "        self.pos_fc = paddle.nn.Linear(in_features=2, out_features=configs.\n",
    "            gat_embed_dim, bias_attr=True)\n",
    "        self.fusion_Attention = AttentionLayer(FullAttention(False, configs\n",
    "            .factor, attention_dropout=configs.dropout, output_attention=\n",
    "            self.output_attention), configs.gat_embed_dim, configs.n_heads)\n",
    "        self.model = paddle.nn.LayerList(sublayers=[TimesBlock(configs) for\n",
    "            _ in range(configs.e_layers)])\n",
    "        self.layer = configs.e_layers\n",
    "        self.layer_norm = paddle.nn.LayerNorm(normalized_shape=configs.d_model)\n",
    "        self.predict_linear = paddle.nn.Linear(in_features=self.seq_len,\n",
    "            out_features=self.pred_len + self.seq_len)\n",
    "        self.projection = paddle.nn.Linear(in_features=configs.d_model,\n",
    "            out_features=configs.c_out, bias_attr=True)\n",
    "\n",
    "    def forecast(self, x_enc, x_mark_enc, x_dec, x_mark_dec):\n",
    "        means = x_enc.mean(axis=1, keepdim=True).detach()\n",
    "        x_enc = x_enc - means\n",
    "        stdev = paddle.sqrt(x=paddle.var(x=x_enc, axis=1, keepdim=True,\n",
    "            unbiased=False) + 1e-05)\n",
    "        x_enc /= stdev\n",
    "        enc_out = self.enc_embedding(x_enc, x_mark_enc)\n",
    "        enc_out = self.predict_linear(enc_out.transpose(perm=[0, 2, 1])\n",
    "            ).transpose(perm=[0, 2, 1])\n",
    "        for i in range(self.layer):\n",
    "            enc_out = self.layer_norm(self.model[i](enc_out))\n",
    "        dec_out = self.projection(enc_out)\n",
    "        dec_out = dec_out * stdev[:, 0, :].unsqueeze(axis=1).repeat(1, self\n",
    "            .pred_len + self.seq_len, 1)\n",
    "        dec_out = dec_out + means[:, 0, :].unsqueeze(axis=1).repeat(1, self\n",
    "            .pred_len + self.seq_len, 1)\n",
    "        return dec_out\n",
    "\n",
    "    def aq_gat(self, G):\n",
    "        x = G.x[:, -self.aq_gat_node_features:].to(self.device)\n",
    "        edge_index = G.edge_index.to(self.device)\n",
    "        edge_attr = G.edge_attr.to(self.device)\n",
    "        g_batch = G.batch.to(self.device)\n",
    "        batch_size = int(len(g_batch) / self.aq_gat_node_num / self.seq_len)\n",
    "        gat_output = self.aq_GAT(x, edge_index, edge_attr, g_batch)\n",
    "        gat_output = gat_output.reshape((batch_size, self.seq_len, self.\n",
    "            aq_gat_node_num, self.gat_embed_dim))\n",
    "        gat_output = paddle.flatten(x=gat_output, start_axis=0, stop_axis=1)\n",
    "        return gat_output\n",
    "\n",
    "    def mete_gat(self, G):\n",
    "        x = G.x[:, -self.mete_gat_node_features:].to(self.device)\n",
    "        edge_index = G.edge_index.to(self.device)\n",
    "        edge_attr = G.edge_attr.to(self.device)\n",
    "        g_batch = G.batch.to(self.device)\n",
    "        batch_size = int(len(g_batch) / self.mete_gat_node_num / self.seq_len)\n",
    "        gat_output = self.mete_GAT(x, edge_index, edge_attr, g_batch)\n",
    "        gat_output = gat_output.reshape((batch_size, self.seq_len, self.\n",
    "            mete_gat_node_num, self.gat_embed_dim))\n",
    "        gat_output = paddle.flatten(x=gat_output, start_axis=0, stop_axis=1)\n",
    "        return gat_output\n",
    "\n",
    "    def norm_pos(self, A, B):\n",
    "        # paddle.mean(x)\n",
    "        A_mean = paddle.mean(A, axis=0)\n",
    "        A_std = paddle.std(A, axis=0)\n",
    "\n",
    "        A_norm = (A - A_mean) / A_std\n",
    "        B_norm = (B - A_mean) / A_std\n",
    "        return A_norm, B_norm\n",
    "\n",
    "    def forward(self, Data, mask=None):\n",
    "        aq_G = Data['aq_G']\n",
    "        mete_G = Data['mete_G']\n",
    "        aq_gat_output = self.aq_gat(aq_G)\n",
    "        mete_gat_output = self.mete_gat(mete_G)\n",
    "        aq_pos, mete_pos = self.norm_pos(aq_G.pos.to(self.device), mete_G.\n",
    "            pos.to(self.device))\n",
    "        aq_pos = self.pos_fc(aq_pos).view(-1, self.aq_gat_node_num, self.\n",
    "            gat_embed_dim)\n",
    "        mete_pos = self.pos_fc(mete_pos).view(-1, self.mete_gat_node_num,\n",
    "            self.gat_embed_dim)\n",
    "        fusion_out, attn = self.fusion_Attention(aq_pos, mete_pos,\n",
    "            mete_gat_output, attn_mask=None)\n",
    "        aq_gat_output = aq_gat_output + fusion_out\n",
    "        aq_gat_output = aq_gat_output.view(-1, self.seq_len, self.\n",
    "            aq_gat_node_num, self.gat_embed_dim)\n",
    "        x = aq_gat_output\n",
    "        perm_0 = list(range(x.ndim))\n",
    "        perm_0[1] = 2\n",
    "        perm_0[2] = 1\n",
    "        aq_gat_output = paddle.transpose(x=x, perm=perm_0)\n",
    "        aq_gat_output = paddle.flatten(x=aq_gat_output, start_axis=0,\n",
    "            stop_axis=1)\n",
    "        train_data = Data['aq_train_data']\n",
    "        x = train_data\n",
    "        perm_1 = list(range(x.ndim))\n",
    "        perm_1[1] = 2\n",
    "        perm_1[2] = 1\n",
    "        train_data = paddle.transpose(x=x, perm=perm_1)\n",
    "        train_data = paddle.flatten(x=train_data, start_axis=0, stop_axis=1)\n",
    "        x_enc = train_data[:, :self.seq_len, -self.dec_in:]\n",
    "        x_mark_enc = train_data[:, :self.seq_len, 1:6]\n",
    "        x_dec = paddle.zeros_like(x=train_data[:, -self.pred_len:, -self.\n",
    "            dec_in:]).astype(dtype='float32')\n",
    "        x_dec = paddle.concat(x=[train_data[:, self.seq_len - self.\n",
    "            label_len:self.seq_len, -self.dec_in:], x_dec], axis=1).astype(\n",
    "            dtype='float32').to(self.device)\n",
    "        x_mark_dec = train_data[:, -self.pred_len - self.label_len:, 1:6]\n",
    "        means = x_enc.mean(axis=1, keepdim=True).detach()\n",
    "        x_enc = x_enc - means\n",
    "        stdev = paddle.sqrt(x=paddle.var(x=x_enc, axis=1, keepdim=True,\n",
    "            unbiased=False) + 1e-05)\n",
    "        x_enc /= stdev\n",
    "        enc_out = self.enc_embedding(aq_gat_output, x_mark_enc)\n",
    "        enc_out = self.predict_linear(enc_out.transpose(perm=[0, 2, 1])\n",
    "            ).transpose(perm=[0, 2, 1])\n",
    "        for i in range(self.layer):\n",
    "            enc_out, period_list, period_weight = self.model[i](enc_out)\n",
    "            enc_out = self.layer_norm(enc_out)\n",
    "        dec_out = self.projection(enc_out)\n",
    "        dec_out = dec_out * stdev[:, 0, :].unsqueeze(axis=1).repeat(1, self\n",
    "            .pred_len + self.seq_len, 1)\n",
    "        dec_out = dec_out + means[:, 0, :].unsqueeze(axis=1).repeat(1, self\n",
    "            .pred_len + self.seq_len, 1)\n",
    "        if self.output_attention:\n",
    "            return dec_out[:, -self.pred_len:, :], attn\n",
    "        else:\n",
    "            return dec_out[:, -self.pred_len:, :], period_list, period_weight\n",
    "\n",
    "\n",
    "def compared_version(ver1, ver2):\n",
    "    \"\"\"\n",
    "    :param ver1\n",
    "    :param ver2\n",
    "    :return: ver1< = >ver2 False/True\n",
    "    \"\"\"\n",
    "    list1 = str(ver1).split('.')\n",
    "    list2 = str(ver2).split('.')\n",
    "    for i in (range(len(list1)) if len(list1) < len(list2) else range(len(\n",
    "        list2))):\n",
    "        if int(list1[i]) == int(list2[i]):\n",
    "            pass\n",
    "        elif int(list1[i]) < int(list2[i]):\n",
    "            return -1\n",
    "        else:\n",
    "            return 1\n",
    "    if len(list1) == len(list2):\n",
    "        return True\n",
    "    elif len(list1) < len(list2):\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "\n",
    "class PositionalEmbedding(paddle.nn.Layer):\n",
    "\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "        pe = paddle.zeros(shape=[max_len, d_model]).astype(dtype='float32')\n",
    "        pe.stop_gradient = True\n",
    "        position = paddle.arange(start=0, end=max_len).astype(dtype='float32'\n",
    "            ).unsqueeze(axis=1)\n",
    "        div_term = (paddle.arange(start=0, end=d_model, step=2).astype(\n",
    "            dtype='float32') * -(math.log(10000.0) / d_model)).exp()\n",
    "        pe[:, 0::2] = paddle.sin(x=position * div_term)\n",
    "        pe[:, 1::2] = paddle.cos(x=position * div_term)\n",
    "        pe = pe.unsqueeze(axis=0)\n",
    "        self.register_buffer(name='pe', tensor=pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.pe[:, :x.shape[1]]\n",
    "\n",
    "\n",
    "class TokenEmbedding(paddle.nn.Layer):\n",
    "\n",
    "    def __init__(self, c_in, d_model):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        padding = 1 if compared_version(paddle.__version__, '1.5.0') else 2\n",
    "        self.tokenConv = paddle.nn.Conv1D(in_channels=c_in, out_channels=\n",
    "            d_model, kernel_size=3, padding=padding, padding_mode=\n",
    "            'circular', bias_attr=False)\n",
    "        for m in self.sublayers():\n",
    "            if isinstance(m, paddle.nn.Conv1D):\n",
    "                init_KaimingNormal = paddle.nn.initializer.KaimingNormal(\n",
    "                    nonlinearity='leaky_relu')\n",
    "                init_KaimingNormal(m.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.tokenConv(x.transpose(perm=[0, 2, 1]))\n",
    "        perm_13 = list(range(x.ndim))\n",
    "        perm_13[1] = 2\n",
    "        perm_13[2] = 1\n",
    "        x = x.transpose(perm=perm_13)\n",
    "        return x\n",
    "\n",
    "\n",
    "class FixedEmbedding(paddle.nn.Layer):\n",
    "\n",
    "    def __init__(self, c_in, d_model):\n",
    "        super(FixedEmbedding, self).__init__()\n",
    "        w = paddle.zeros(shape=[c_in, d_model]).astype(dtype='float32')\n",
    "        w.stop_gradient = True\n",
    "        position = paddle.arange(start=0, end=c_in).astype(dtype='float32'\n",
    "            ).unsqueeze(axis=1)\n",
    "        div_term = (paddle.arange(start=0, end=d_model, step=2).astype(\n",
    "            dtype='float32') * -(math.log(10000.0) / d_model)).exp()\n",
    "        w[:, 0::2] = paddle.sin(x=position * div_term)\n",
    "        w[:, 1::2] = paddle.cos(x=position * div_term)\n",
    "        self.emb = paddle.nn.Embedding(num_embeddings=c_in, embedding_dim=\n",
    "            d_model)\n",
    "        out_3 = paddle.create_parameter(shape=w.shape, dtype=w.numpy().\n",
    "            dtype, default_initializer=paddle.nn.initializer.Assign(w))\n",
    "        out_3.stop_gradient = not False\n",
    "        self.emb.weight = out_3\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.emb(x).detach()\n",
    "\n",
    "\n",
    "class TemporalEmbedding(paddle.nn.Layer):\n",
    "\n",
    "    def __init__(self, d_model, embed_type='fixed', freq='h'):\n",
    "        super(TemporalEmbedding, self).__init__()\n",
    "        minute_size = 4\n",
    "        hour_size = 24\n",
    "        weeknum_size = 53\n",
    "        weekday_size = 7\n",
    "        day_size = 32\n",
    "        month_size = 13\n",
    "        Embed = (FixedEmbedding if embed_type == 'fixed' else paddle.nn.\n",
    "            Embedding)\n",
    "        if freq == 't':\n",
    "            self.minute_embed = Embed(minute_size, d_model)\n",
    "        self.hour_embed = Embed(hour_size, d_model)\n",
    "        self.weekday_embed = Embed(weekday_size, d_model)\n",
    "        self.weeknum_embed = Embed(weeknum_size, d_model)\n",
    "        self.day_embed = Embed(day_size, d_model)\n",
    "        self.month_embed = Embed(month_size, d_model)\n",
    "        self.Temporal_feature = ['month', 'day', 'week', 'weekday', 'hour']\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.astype(dtype='int64')\n",
    "        for idx, freq in enumerate(self.Temporal_feature):\n",
    "            if freq == 'year':\n",
    "                pass\n",
    "            elif freq == 'month':\n",
    "                month_x = self.month_embed(x[:, :, idx])\n",
    "            elif freq == 'day':\n",
    "                day_x = self.day_embed(x[:, :, idx])\n",
    "            elif freq == 'week':\n",
    "                weeknum_x = self.weeknum_embed(x[:, :, idx])\n",
    "            elif freq == 'weekday':\n",
    "                weekday_x = self.weekday_embed(x[:, :, idx])\n",
    "            elif freq == 'hour':\n",
    "                hour_x = self.hour_embed(x[:, :, idx])\n",
    "        return hour_x + weekday_x + weeknum_x + day_x + month_x\n",
    "\n",
    "\n",
    "class TimeFeatureEmbedding(paddle.nn.Layer):\n",
    "\n",
    "    def __init__(self, d_model, embed_type='timeF', freq='h'):\n",
    "        super(TimeFeatureEmbedding, self).__init__()\n",
    "        freq_map = {'h': 4, 't': 5, 's': 6, 'm': 1, 'a': 1, 'w': 2, 'd': 3,\n",
    "            'b': 3}\n",
    "        d_inp = freq_map[freq]\n",
    "        self.embed = paddle.nn.Linear(in_features=d_inp, out_features=\n",
    "            d_model, bias_attr=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embed(x)\n",
    "\n",
    "\n",
    "class DataEmbedding(paddle.nn.Layer):\n",
    "\n",
    "    def __init__(self, c_in, d_model, embed_type='fixed', freq='h', dropout=0.1\n",
    "        ):\n",
    "        super(DataEmbedding, self).__init__()\n",
    "        self.value_embedding = TokenEmbedding(c_in=c_in, d_model=d_model)\n",
    "        self.position_embedding = PositionalEmbedding(d_model=d_model)\n",
    "        self.temporal_embedding = TemporalEmbedding(d_model=d_model,\n",
    "            embed_type=embed_type, freq=freq\n",
    "            ) if embed_type != 'timeF' else TimeFeatureEmbedding(d_model=\n",
    "            d_model, embed_type=embed_type, freq=freq)\n",
    "        self.dropout = paddle.nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x, x_mark):\n",
    "        x = self.value_embedding(x) + self.temporal_embedding(x_mark\n",
    "            ) + self.position_embedding(x)\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "\n",
    "class DataEmbedding_wo_pos(paddle.nn.Layer):\n",
    "\n",
    "    def __init__(self, c_in, d_model, embed_type='fixed', freq='h', dropout=0.1\n",
    "        ):\n",
    "        super(DataEmbedding_wo_pos, self).__init__()\n",
    "        self.value_embedding = TokenEmbedding(c_in=c_in, d_model=d_model)\n",
    "        self.position_embedding = PositionalEmbedding(d_model=d_model)\n",
    "        self.temporal_embedding = TemporalEmbedding(d_model=d_model,\n",
    "            embed_type=embed_type, freq=freq\n",
    "            ) if embed_type != 'timeF' else TimeFeatureEmbedding(d_model=\n",
    "            d_model, embed_type=embed_type, freq=freq)\n",
    "        self.dropout = paddle.nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x, x_mark):\n",
    "        x = self.value_embedding(x) + self.temporal_embedding(x_mark)\n",
    "        return self.dropout(x)\n",
    "\n",
    "class GAT_Encoder(paddle.nn.Layer):\n",
    "\n",
    "    def __init__(self, input_dim, hid_dim, edge_dim, gnn_embed_dim, dropout):\n",
    "        super(GAT_Encoder, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.relu = paddle.nn.ReLU()\n",
    "        self.dropout = paddle.nn.Dropout(p=dropout)\n",
    "        self.conv1 = GATv2Conv(input_dim, hid_dim, )\n",
    "        self.conv2 = GATv2Conv(hid_dim, hid_dim * 2, )\n",
    "        self.conv3 = GATv2Conv(hid_dim * 2, gnn_embed_dim,)\n",
    "\n",
    "    def forward(self,graph, feature, ):\n",
    "        x = self.conv1(graph, feature )\n",
    "        x = x.relu()\n",
    "        x = self.conv2(graph, x)\n",
    "        x = x.relu()\n",
    "        x = self.conv3(graph, x)\n",
    "        x = self.dropout(x)\n",
    "       \n",
    "        return x       \n",
    "class GATv2Conv(nn.Layer):\n",
    "    \"\"\"Implementation of How Attentive are Graph Attention Networks?\n",
    "\n",
    "    This is an implementation of the paper HOW ATTENTIVE ARE GRAPH ATTENTION NETWORKS?\n",
    "    (https://arxiv.org/pdf/2105.14491.pdf).\n",
    "\n",
    "    Args:\n",
    "\n",
    "        input_size: The size of the inputs.\n",
    "\n",
    "        hidden_size: The hidden size for gat.\n",
    "\n",
    "        activation: (default None) The activation for the output.\n",
    "\n",
    "        num_heads: (default 1) The head number in gat.\n",
    "\n",
    "        feat_drop: (default 0.6) Dropout rate for feature.\n",
    "\n",
    "        attn_drop: (default 0.6) Dropout rate for attention.\n",
    "\n",
    "        concat: (default True) Whether to concat output heads or average them.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size,\n",
    "        hidden_size,\n",
    "        feat_drop=0.6,\n",
    "        attn_drop=0.6,\n",
    "        num_heads=1,\n",
    "        concat=True,\n",
    "        activation=None,\n",
    "    ):\n",
    "        super(GATv2Conv, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_heads = num_heads\n",
    "        self.feat_drop = feat_drop\n",
    "        self.attn_drop = attn_drop\n",
    "        self.concat = concat\n",
    "\n",
    "        self.linear = nn.Linear(input_size, num_heads * hidden_size)\n",
    "        self.attn = self.create_parameter(shape=[1, num_heads, hidden_size])\n",
    "        self.feat_dropout = nn.Dropout(p=feat_drop)\n",
    "        self.attn_dropout = nn.Dropout(p=attn_drop)\n",
    "        self.leaky_relu = nn.LeakyReLU(negative_slope=0.2)\n",
    "\n",
    "        if isinstance(activation, str):\n",
    "            activation = getattr(F, activation)\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, graph, feature):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "\n",
    "            graph: `pgl.Graph` instance.\n",
    "\n",
    "            feature: A tensor with shape (num_nodes, input_size)\n",
    "\n",
    "        Return:\n",
    "\n",
    "            If `concat=True` then return a tensor with shape (num_nodes, hidden_size),\n",
    "            else return a tensor with shape (num_nodes, hidden_size * num_heads)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        if self.feat_drop > 1e-15:\n",
    "            feature = self.feat_dropout(feature)\n",
    "\n",
    "        feature = self.linear(feature)\n",
    "        feature = paddle.reshape(feature, [-1, self.num_heads, self.hidden_size])\n",
    "        alpha = graph.send_uv(feature, feature, \"add\")\n",
    "        alpha = self.leaky_relu(alpha)\n",
    "        alpha = paddle.sum(alpha * self.attn, axis=-1)\n",
    "        alpha = GF.edge_softmax(graph, alpha)\n",
    "        alpha = paddle.reshape(alpha, [-1, self.num_heads, 1])\n",
    "        if self.attn_drop > 1e-15:\n",
    "            alpha = self.attn_dropout(alpha)\n",
    "        output = graph.send_ue_recv(feature, alpha, \"mul\", \"sum\")\n",
    "        if self.concat:\n",
    "            output = paddle.reshape(output, [-1, self.num_heads * self.hidden_size])\n",
    "        else:\n",
    "            output = paddle.mean(output, axis=1)\n",
    "\n",
    "        if self.activation is not None:\n",
    "            output = self.activation(output)\n",
    "        return output\n",
    "    \n",
    "Gat_TimesNet_mm(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-05-07 12:42:30,179] [ WARNING] install_check.py:71 - You are using GPU version PaddlePaddle, but there is no GPU detected on your machine. Maybe CUDA devices is not set properly.\n",
      " Original Error is \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running verify PaddlePaddle program ... \n",
      "PaddlePaddle works well on 1 CPU.\n",
      "PaddlePaddle works well on 2 CPUs.\n",
      "PaddlePaddle is installed successfully! Let's start deep learning with PaddlePaddle now.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0507 12:42:30.252836 18901 fuse_all_reduce_op_pass.cc:79] Find all_reduce operators: 2. To make the speed faster, some all_reduce ops are fused during training, after fusion, the number of all_reduce ops is 2.\n"
     ]
    }
   ],
   "source": [
    "import paddle\n",
    "import os\n",
    "# 仅设置一块可见\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "paddle.utils.run_check()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paddle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
